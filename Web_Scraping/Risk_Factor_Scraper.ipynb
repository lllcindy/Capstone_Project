{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Risk_Factor_Scraper.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"0QW0Mboq2dhw","colab":{}},"source":["import requests\n","from lxml import html, etree\n","from lxml.etree import tostring\n","from bs4 import BeautifulSoup\n","import re\n","import csv\n","import pandas as pd\n","from tqdm import tqdm_notebook\n","from datetime import datetime\n","import collections\n","from collections import defaultdict\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9jmIEnvF2dh6"},"source":["# Utils\n","#### These functions are applicable for both 10K and 10Q such as removing foot note, getting url for reports and requesting the url to get the content"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a6edoNDs2dh8","colab":{}},"source":["# Define some global variables\n","BASE_URL = \"https://www.sec.gov\"\n","n = datetime.now()\n","default_year = [n.year, n.year-1, n.year-2]\n","\n","# Create the folder \"risk_factors\" if not already exist\n","if not os.path.exists('risk_factors'):\n","    os.makedirs('risk_factors')\n","\n","def get_files(cik, doc_type, no_of_documents=1, debug=False):\n","    '''\n","    Getting the text file url from sec using cik number\n","    Args:\n","        cik : company code\n","        no_of_documents: default 1\n","    \n","    Returns: \n","        String of url\n","    '''\n","    if doc_type not in ['10-K', '10-Q']:\n","        print(\"Input doc type should be '10-K' or '10-Q'\")\n","        return\n","    url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}\"\n","    href = \"\"\n","    prior_to=\"\"\n","    ownership=\"include\"\n","    no_of_entries=100\n","    filing_type= doc_type\n","    url1 = url + \"&type=\" + filing_type + \"&dateb=\" + prior_to + \"&owner=\" +  ownership + \"&count=\" + str(no_of_entries)\n","    page = requests.get(url1, timeout=10)\n","    tree = html.fromstring(page.content)\n","    elems = tree.xpath('//*[@id=\"documentsbutton\"]')[:no_of_documents]\n","    result = []\n","    \n","    # Use the base url, we first enter the page of table of documents. Get the last one url of the table. That url is what we want\n","    for elem in elems:\n","        url2 = BASE_URL + elem.attrib[\"href\"]\n","        content_page = get_request(url2)\n","        table = content_page.find_class(\"tableFile\")[0]\n","        last_row = table.getchildren()[-1]\n","        href = last_row.getchildren()[2].getchildren()[0].attrib[\"href\"]\n","        href = BASE_URL + href\n","        result.append(href)\n","    return result\n","\n","def get_request(href, isxml=False):\n","    '''\n","    Get the page content given url\n","    Args:\n","        href : given url\n","    Returns: \n","        String of page content\n","    '''\n","    page = requests.get(href)\n","    if isxml:\n","        p = etree.XMLParser(huge_tree=True)\n","        return etree.fromstring(page.content, parser=p)\n","    else:\n","        return html.fromstring(page.content)\n","    \n","\n","\n","def is_number(s):\n","    '''\n","    Check whether the string is a number\n","    Args:\n","        s : the given string\n","    Returns: \n","        Bool : whether the string is a number\n","    '''\n","    try:\n","        float(s)\n","        return True\n","    except ValueError:\n","        pass\n"," \n","    try:\n","        import unicodedata\n","        unicodedata.numeric(s)\n","        return True\n","    except (TypeError, ValueError):\n","        pass\n"," \n","    return False\n","\n","def remove_footnote(text):\n","    '''\n","    Remove the page number in foot note\n","    Args:\n","        text : extracted risk factor part\n","    Returns: \n","        String of the risk factor part after removing the page number\n","    '''\n","    sentence = text.split(\"\\n\")\n","    text_new = \"\"\n","    for i in sentence:\n","        if is_number(i) and len(i) < 4:\n","            continue\n","        text_new += i + \"\\n\"\n","    return text_new"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dbMTcmDk2diB"},"source":["## 10_K\n","#### Following functions are specifically written to scrape the 'Risk factors' section of 10k reports from Edgar database\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LbtJl5t92diD","colab":{}},"source":["def get_company_url_10k(company_list=None, year=default_year):\n","    '''\n","    getting list of urls for all 10Ks\n","    Args:\n","        company_list : default is all company on SEC, or user can provide\n","        year: default is the nearest 3 year\n","    Returns: \n","        list of tuples: (ticker, url)\n","    '''\n","    ticker_dict = dict() # a dictionary to store the url\n","    \n","    # read the ticker.txt downloaded form the SEC at: https://www.sec.gov/about/webmaster-faq.htm#developers\n","    # read the file into dictionary as {ticker: cik_code}\n","    with open('ticker.txt', mode='r') as infile:\n","        for line in infile:\n","            item = line.split('\\t')\n","            ticker_dict[item[0]] = item[1].strip().zfill(7) #left padding with zero\n","            \n","        # if company_list is not provided, stored the whole dictionary to a list of tuple as (ticker, cik_code)\n","        if not company_list:\n","            mylist = [(item[0], item[1]) for item in ticker_dict.items()]\n","        # if the company_list is provided, only get the cik codes of those company provided     \n","        else:\n","            mylist = [(ticker, ticker_dict[ticker]) for ticker in company_list]\n","            \n","    url_dict = {} # temp dict to store the urls as {ticker: [list of urls]}\n","    n = datetime.now() # get the current date to compute the number of documents to be scraped\n","    for i in tqdm_notebook(mylist):\n","        urlk = get_files(i[1], \"10-K\", no_of_documents=(n.year - min(year))*2) # considering the possibility of having an amendeded report, ex. we get 6 documents for 3 years to avoid missing some reports\n","        url_dict[i[0]] = urlk\n","    url_10k = []\n","    year.append(max(year)+1) # some company may have 10k filing in the later year (ex. 2018Q4 filled in 2019) to avoid missing the report we add 1 to the max year for scraping\n","    for k,v in url_dict.items():\n","        for i in v:\n","            year_k = int(i.split(\"-\")[1])+2000\n","            # get rid of reports not in the range of input or default year\n","            if year_k in year:\n","                url_10k.append((k, i))\n","    return url_10k\n","\n","def parser_10k(url):\n","    '''\n","    parse the 10k report\n","    Args:\n","        url : 10K report url\n","    Returns: \n","        String of the 10k document\n","    '''\n","    r = requests.get(url)\n","    raw_10k = r.text\n","    # regex to find <DOCUMENT> tags\n","    doc_start_pattern = re.compile(r'<DOCUMENT>')\n","    doc_end_pattern = re.compile(r'</DOCUMENT>')\n","    # regex to find <TYPE> tag prceeding any characters, terminating at new line\n","    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n","    # regex to get the filling date\n","    date_pattern = re.compile(r'FILED AS OF DATE:.+')\n","    # create 3 lists with the span idices for each regex\n","    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n","    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n","    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n","    # get the filling date\n","    doc_date = [x[len('FILED AS OF DATE:\\t\\t'):] for x in date_pattern.findall(raw_10k)][0][:8]\n","    \n","    document = {}\n","\n","    # Create a loop to go through each section type and save only the 10-K & 10-K/A section in the dictionary\n","    d_type = \"\"\n","    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n","        if doc_type == '10-K' or doc_type == '10-K/A' or doc_type == '10-KT' or doc_type == '10-KT/A':\n","            document[doc_type] = raw_10k[doc_start:doc_end]\n","            d_type = doc_type\n","\n","    return document[d_type], d_type, doc_date\n","\n","def get_rf_section_10k(text):\n","    '''\n","    get the text of the risk factor section\n","    the pattern for regular expression failed to handle some exceptions, such as companies having their risk factor section under different title other than item 1A, or having rare spacing characters.\n","    Args:\n","        String : raw text of the 10-K returned by the parse 10-K function\n","    Returns: \n","        String of raw risk factor section\n","    '''\n","    # write the regex\n","    regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;|\\n    )(1a|1A|1b|1B|2|3|4)\\.{0,1})|(ITEM(\\s|&#160;|&nbsp;|\\n    )(1a|1A|1b|1B|2|3|4))|(PART II)|(>tem(&#160;| )(1A|1B|2|3|4))|(>TEM(&#160;| )(1A|1B|2|3|4))|((>&#32;Item(\\s|&#160;|&nbsp;|\\n    )(1a|1A|1b|1B|2|3|4)))')\n","\n","    # use finditer to match the regex\n","    matches = regex.finditer(text)\n","    \n","    # store all matches\n","    all_match = []\n","    for match in matches:\n","        all_match.append(match)\n","    \n","    # return \"No Risk Factor Part\" if no matches\n","    if len(all_match) == 0:\n","        return \"No Risk Factor Part\"\n","    \n","    matches = regex.finditer(text)\n","    # create the dataframe\n","    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n","    test_df.columns = ['item', 'start', 'end']\n","    test_df['item'] = test_df.item.str.lower()\n","\n","    \n","    # get rid of unnesesary charcters from the dataframe\n","    test_df.replace('&#160;',' ',regex=True,inplace=True)\n","    test_df.replace('&#32;','',regex=True,inplace=True)\n","    test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n","    test_df.replace(' ','',regex=True,inplace=True)\n","    test_df.replace('\\.','',regex=True,inplace=True)\n","    test_df.replace('>','',regex=True,inplace=True)\n","    test_df.replace('\\n','',regex=True,inplace=True)\n","    test_df.replace('\\n    ','',regex=True,inplace=True)\n","    test_df.replace('^tem','item',regex=True,inplace=True)\n","    test_df.replace('iitem','item',regex=True,inplace=True)\n","\n","    # sort all the items in the dataframe, drop duplicated items and only keep last item with the same name\n","    if \"item1a\" in test_df[\"item\"].values:\n","        pos_dat = test_df.loc[test_df['item'].shift(1) != test_df['item']]\n","        pos_dat = pos_dat.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n","        pos_dat = pos_dat.reset_index()\n","       \n","        item_1a_index = pos_dat[pos_dat['item']=='item1a'].index.values[0]\n","        if item_1a_index != pos_dat.shape[0]-1:\n","            item_1a_raw = text[pos_dat['start'].loc[item_1a_index]:pos_dat['start'].loc[item_1a_index + 1]]\n","        else:\n","             item_1a_raw = \"No Risk Factor Part\"\n","\n","    else:\n","        item_1a_raw = \"No Risk Factor Part\"\n","    \n","    return item_1a_raw\n","\n","def content_refine_10k(text):\n","    '''\n","    Refine the raw risk factor text.\n","    Args:\n","        String : raw risk factor String returned by method \"get_rf_section_10k\"\n","    Returns: \n","        String of refined risk factor section\n","    '''\n","    if not text:\n","        return None\n","    # First convert the raw text we have to exrtacted to BeautifulSoup object \n","    item_1a_content = BeautifulSoup(text, 'lxml')\n","    \n","    # Method get_text() is what we need, \\n\\n is optional, I just added this to read text \n","    # more cleanly, it's basically new line character between sections. \n","    return item_1a_content.get_text(\"\\n\\n\")\n","\n","def get_rf_10k(url_10k, item_dict, year):\n","    '''\n","    Combine report processing methods above into single function and write the result to text file in risk_factors file.\n","    Args:\n","        List : list of url for 10Ks\n","        Dict : dictionary to keep track of written files(avoid duplicate file e.g. if amend file has risk factor section, we take the content in the amend file)\n","        List : list of years to be scraped (default - the recent 3 year)\n","    '''\n","    # get the raw text of the full report with the doc_type and doc_date\n","    raw_text, doc_type, doc_date = parser_10k(url_10k[1])\n","    # get document year\n","    doc_year = int(doc_date[:4])\n","    # if document year is not in year list return None\n","    if doc_year not in year:\n","        return None\n","    # get the raw risk factor text\n","    raw_rf_text = get_rf_section_10k(raw_text)\n","    # if amend report does not contain risk factor section, return None\n","    if raw_rf_text == \"No Risk Factor Part\" and doc_type == \"10-K/A\":\n","        return None\n","    # refine the raw risk factor text\n","    rf = content_refine_10k(raw_rf_text)\n","    # remove footnote\n","    rf_ = remove_footnote(rf)\n","    # get company ticker stored previously in url_10k\n","    company = url_10k[0]\n","    # get the company name and report year and store as String for future purpose in keeping track of the text written to output directory\n","    file = company+'_' + str(doc_date)\n","    # if file name already exist(since amend report will be scraped first due to the ordering, meaning the amend report of the year contains risk factor section\n","    # and the current file with the duplicate name should be the original 10-K, so we store the file with name \"ticker_filling date_10k_org\")\n","    if file in item_dict.keys():\n","        file_name = \"risk_factors/{0}_{1}_10k_org.txt\".format(company, str(doc_date))\n","    file_name = \"risk_factors/{0}_{1}_10k.txt\".format(company, str(doc_date))\n","    with open(file_name, 'w', encoding='utf-8') as f: \n","        f.write(rf_)      \n","    return file\n","\n","def scraper_10K(company_list=None, year=default_year):\n","    '''\n","    Combine the code for getting 10K urls and text processing.\n","    Args:\n","        List : company list containing tickers of the companies to be scraped(default: all company)\n","        List : year list containing years to be scraped(default: most recent 3 years)\n","    Returns: \n","        List of failed cases.\n","    '''\n","    # get the urls\n","    url_10k = get_company_url_10k(company_list, year)\n","    # create a dictionary to keep track on scraped reports\n","    item_dict = collections.defaultdict(int)\n","    # create a list to store failed cases\n","    failed_case = []\n","    # loop through the urls to get all 10Ks' risk factor section\n","    for item in tqdm_notebook(url_10k):\n","        try:\n","            file = get_rf_10k(item, item_dict, year)\n","            if not file:\n","                continue\n","            item_dict[file] += 1\n","        except:\n","            failed_case.append(item)\n","    return failed_case"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q4c9Eg1HKNkK"},"source":["## 10_Q\n","#### Following functions are specifically written to scrape the 'Risk factors' section of 10Q reports from Edgar database"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s9_SArxpKNkL","colab":{}},"source":["def get_company_url_10q(ticker=None, yearl=default_year):\n","    '''\n","    Get urls of 10q files for certain tickers and years\n","    Args:\n","        ticker : given ticker list\n","        yearl : given year list \n","    Returns: \n","        List of urls of 10q files\n","    '''\n","    mydict = {}\n","    # read the file that matches the tickers and cik number, since we use cik number to get the url\n","    with open('ticker.txt', mode='r') as infile:\n","        for line in infile:\n","            item = line.split('\\t')\n","            mydict[item[0]] = item[1].strip().zfill(7)\n","    if ticker == None:\n","        ticker = mydict.keys()\n","    url_10q=[]\n","    curr_year = datetime.now().year\n","    for i in tqdm_notebook(ticker):\n","        diff = curr_year + 1 - min(yearl)\n","        # use the get_files function to get the certain number of urls;\n","        # the number is the difference between the earliest year multiplies 6\n","        # (in one year, the maximum number of 10q is 3 quarters * 2 types of 10q -- 10q and 10q/a)\n","        urlq = get_files(mydict[i], \"10-Q\", no_of_documents = diff*3*2)\n","        for j in range(len(urlq)):\n","            if urlq[j] != \"\":\n","                yearq = int(urlq[j].split(\"-\")[1]) + 2000\n","                # to check whether the year of the document is within the given list of years\n","                if yearq in yearl:\n","                    url_10q.append((i, urlq[j], yearq))\n","    return url_10q\n","\n","def parser_10q(url):\n","    '''\n","    Extract the raw text of 10q file\n","    Args:\n","        url : 10q report url\n","    Returns: \n","        String of the 10q document, document type and filling date\n","    '''\n","    r = requests.get(url)\n","    raw_10q = r.text\n","    # get the fillling date of the 10q file\n","    line = raw_10q.split(\"\\n\")\n","    date = line[7].split(\"\\t\")\n","    # Regex to find <DOCUMENT> tags\n","    doc_start_pattern = re.compile(r'<DOCUMENT>')\n","    doc_end_pattern = re.compile(r'</DOCUMENT>')\n","    # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n","    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n","    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10q)]\n","    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10q)]\n","    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10q)]\n","    document = {}\n","    # get the document type and the main content within this tag; we only want the document type of 10q or 10q/a\n","    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n","        if doc_type == '10-Q':\n","            document[doc_type] = raw_10q[doc_start:doc_end]\n","            return document['10-Q'], '10-Q', date[-1]\n","\n","        if doc_type == '10-Q/A':\n","            document[doc_type] = raw_10q[doc_start:doc_end]\n","            return document['10-Q/A'], '10-Q/A', date[-1]\n","\n","    return None, None, None\n","\n","def get_rf_section_10q(text):\n","    '''\n","    Use regular experssion to extract risk factor part in the raw text (However, our regular expression is hard to capture all the risk factor structures)\n","    Args:\n","        text : extracted raw text\n","    Returns: \n","        String of the risk factor part or \"No Risk Factor Part\"\n","    '''\n","    item_1a = \"\"\n","    if text == None:\n","        return \"No Risk Factor Part\"\n","    # Write the regex (This regex now cannot capture all the risk fatcor patterns, since there are a lot of different structures of documents and we now check them manuallyã€‚)\n","    regex = re.compile(r'(> *Item(\\s|&#160;|&nbsp;|&#32;)(1A|1B|2|3|4|5|6)\\.{0,1})|(> *ITEM(\\s|&#160;|&nbsp;|&#32;)(1A|1B|2|3|4|5|6))')\n","    matches = regex.finditer(text)\n","\n","    try:\n","        # Create the dataframe and store the item name, start position and end position of this item\n","        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n","        test_df.columns = ['item', 'start', 'end']\n","        test_df['item'] = test_df.item.str.lower()\n","\n","        # Get rid of unnesesary charcters from the dataframe\n","        test_df.replace('&#160;',' ',regex=True,inplace=True)\n","        test_df.replace('&#32;','',regex=True,inplace=True)\n","        test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n","        test_df.replace(' ','',regex=True,inplace=True)\n","        test_df.replace('\\.','',regex=True,inplace=True)\n","        test_df.replace('>','',regex=True,inplace=True)\n","        test_df.replace('\\n','',regex=True,inplace=True)\n","        test_df.replace('\\n    ','',regex=True,inplace=True)\n","        test_df.replace('^tem','item',regex=True,inplace=True)\n","        test_df.replace('iitem','item',regex=True,inplace=True)\n","\n","        # sort all the items in the dataframe, drop duplicated items and only keep last item with the same name\n","        if \"item1a\" in test_df[\"item\"].values:\n","            pos_dat = test_df.loc[test_df['item'].shift(1) != test_df['item']]\n","            pos_dat = pos_dat.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n","            pos_dat = pos_dat.reset_index()       \n","            item_1a_index = pos_dat[pos_dat['item']=='item1a'].index.values[0]\n","            # if the certain item cannot be found, just return \"No Risk factor Part\"\n","            if item_1a_index != pos_dat.shape[0]-1:\n","                item_1a = text[pos_dat['start'].loc[item_1a_index]:pos_dat['start'].loc[item_1a_index + 1]]\n","            else:\n","                 item_1a = \"No Risk Factor Part\"\n","\n","        else:\n","            item_1a = \"No Risk Factor Part\"\n","            \n","    except:\n","        item_1a = \"No Risk Factor Part\"\n","    \n","    return item_1a\n","\n","def content_refine_10q(raw_text):\n","    '''\n","    Refine the risk factor part with the original format with paragraph\n","    Args:\n","        raw_text : extracted risk factor part\n","    Returns: \n","        String of the refined risk factor part\n","    '''\n","    if raw_text == \"No Risk Factor Part\":\n","        return raw_text\n","    # Convert the raw text we have to exrtacted to BeautifulSoup object \n","    item_1a_content = BeautifulSoup(raw_text, 'lxml')\n","    return item_1a_content.get_text(\"\\n\")\n","\n","def write_10q(outputfile, text):\n","    '''\n","    Write the risk factor in file\n","    Args:\n","        outputfile : the name of the output file\n","        text : extracted text to be written\n","    '''\n","    with open(outputfile, 'w', encoding='utf-8') as f:\n","        f.write(text)\n","        \n","def get_rf_10q(url_10q, flag):\n","    '''\n","    Integrate the functions to get the risk factor by one given url\n","    Args:\n","        url_10q : one url of the 10q file\n","        flag : integer used to mark whether we will pass the next file\n","    Returns: \n","        String of the output file name and the current flag\n","    '''\n","    # Define the path of the files\n","    base = \"risk_factors/\"\n","    # replace the \"/\" in name to \"_\"\n","    name = url_10q[0].replace(\"/\", \"_\")\n","    raw_text, doctype, date = parser_10q(url_10q[1])\n","    raw_rf = get_rf_section_10q(raw_text)\n","    \n","    # After we get the risk factor part, there is a strategy to get the correct risk factor:\n","    # if there is no risk factor part in 10q/a, we should find the next file(10q), so set a flag to mark this.\n","    # if there is risk factor part in 10q/a, just write this part into output and pass the next file, also set a flag to mark this.\n","    # if the type is 10q, no matter what we get, write the result into output.\n","    if raw_rf == \"No Risk Factor Part\" and (doctype == \"10-Q/A\" or doctype == None) and flag != -1:\n","        return (None, -1)\n","    elif raw_rf !=\"No Risk Factor Part\" and doctype == \"10-Q/A\":\n","        flag = 1\n","        \n","    flag = 0\n","    file_name = base+name+\"_\"+date+\"_10q.txt\"\n","    if raw_rf == \"No Risk Factor Part\":\n","        write_10q(file_name, raw_rf)\n","    else:\n","        # Refine the risk factor part and remove the page number in foot note.\n","        rf = content_refine_10q(raw_rf)\n","        rf = remove_footnote(rf)\n","        write_10q(file_name, rf)\n","    return file_name, flag\n","\n","def scraper_10Q(ticker=None, yearl=default_year):\n","    '''\n","    Put all the functions of 10q part together and write all the scraping result\n","    Args:\n","        ticker : given ticker list\n","        yearl : given year list \n","    Returns: \n","        List of failed cases\n","    '''\n","    failed_case = []\n","    filenames = {}\n","    totalnumber = {}\n","    flag = 0\n","    # get all the urls for given input\n","    url_10q = get_company_url_10q(ticker, yearl)\n","    \n","    # Since sometimes we need to pass some file(as described in function \"get_rf_10q\"),\n","    # and ensure there are no more than 3 files in the year of given ticker, we need to \n","    # use a dictinary filenames to count the number of files with the same name in the same year\n","    for item in tqdm_notebook(url_10q):\n","        name = item[0]\n","        year = item[2]\n","        if name not in filenames:\n","            filenames[name] = {}\n","            filenames[name][year] = 0\n","            flag = 0\n","        else:\n","            if year not in filenames[name]:\n","                filenames[name][year] = 0\n","                flag = 0\n","            else:\n","                filenames[name][year] += 1\n","                \n","        if  filenames[name][year] >= 3:\n","            continue\n","              \n","        # if the flag is 1 in the last round, we should pass the current file.\n","        if flag == 1:\n","            flag = 0\n","            continue\n","        try:\n","            file, flag = get_rf_10q(item, flag)\n","            # if the flag is 1 or -1, that means we pass one file, so the count should be -1.\n","            if flag == -1 or flag == 1:\n","                filenames[name][year] -= 1\n","            if file == None:\n","                continue\n","        # if there are some exceptions, just put them into failed_case list\n","        except:\n","            failed_case.append(item)\n","    return failed_case"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lhHFGfrsKNkO"},"source":["## Get All Risk Factors"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wzgJoQt9KNkP","colab":{}},"source":["def Risk_Factor_Scraper(company_list=None, year=default_year):\n","    '''\n","    This function integrate the 10k scraper and 10q scraper\n","    Args:\n","        comapny_list : given ticker list\n","        year : given year list \n","    Returns: \n","        List of 10k failed cases and list of 10q failed cases\n","    '''\n","    failed_case_10q = scraper_10Q(company_list, year)\n","    failed_case_10k = scraper_10K(company_list, year)\n","    return failed_case_10k, failed_case_10q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tDiClMvxKNkR","outputId":"7bc2d946-65c4-430e-8264-c38fc93becf2","colab":{"referenced_widgets":["8ade8eb39cce4789a4937c48453b3cf0","d14b428d3e5b4108a845af4b899c1f17","1c25f174941c4c098aa16a4135ee11be","3d646441dad0415cbe6373820e3882fe","b5b06b3b87f847be9a85d68a8fe88a85","6691b0bd02ac4866aebb3db31e26b7e6","a1478a1a2dcb4de8a84bd5e18f82a3e0","d7e712a14fa54f6b915347c11ee4da63","e3d7508e376c442aac9cc8be4818d79b","d31c950865844e0f87ea2dff4fe32421","5e8accd683c242fc95ed772ad124a2d4","7c2dcee2e1f44a77b824f35d39baed5d"]}},"source":["# In pratice, you can just change the input -- ticker_list and year_list, \n","# and run this function, the certain risk factor part will be written into the folder\n","ticker_list = [\"aapl\"]\n","year_list = [2018, 2019, 2020]\n","\n","failed_case_10k, failed_case_10q = Risk_Factor_Scraper(ticker_list)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3d7508e376c442aac9cc8be4818d79b","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d31c950865844e0f87ea2dff4fe32421","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e8accd683c242fc95ed772ad124a2d4","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c2dcee2e1f44a77b824f35d39baed5d","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"91oZ9KKNB-Bp","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}